---
title: "Working with count compositional data"
author: "SK"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: cayman
    toc: true
    toc_depth: 2
  code_folding: hide

# output:
#   tufte::tufte_html: 
#     toc: true
#     toc_depth: 2
#   code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```


```{r, eval=TRUE, echo=F, message=F, warning=F, comment=F}
library(rstan, quietly=T, verbose = F)
rstan_options(auto_write = TRUE)
library(ggplot2)
require(patchwork)
```


# Introduction
Dirichlet multinomial ($DM$) distribution is a compound probability distribution,
where a probability vector $\theta$ is drawn from a Dirichlet distribution with
parameter vector $\alpha$ and an observation data (counts; $y$) drawn from a 
multinomial distribution with probability vector $\theta$ and total number of 
trials $n$. The Dirichlet prior - the vector $\alpha\in\mathbb{R}^{+}$, can be 
seen as **pseudo**count:
  
  * $y \sim \mathrm{Multinomial}(\theta, n)$
  * $\theta \sim \mathrm{Dirichlet}(\alpha)$
  * $\alpha = \ldots$

Crash course on Dirichlet and Multinomial distributions: 
https://ixkael.github.io/dirichlet-multinomial-conjugate-priors/


# Lets simulate data from DM distribution 
Here I assume we have samples obtained from a number of patients before 
and after treatment. Each sample measures mRNA abundances (**counts**) of 
$K$ genes (categories). Hence, sample $i$ is represented by $K$-vector
$\vec{y}_{i}=\{y_{i1},y_{i2},\ldots,y_{ik}\}$. 

For patient $i$ we have two such samples: 

  * $\vec{y}_i^{b}$: collected before treatment
  * $\vec{y}_i^{a}$: collected after treatment

The sum of the counts in each sample is fixed to $n_i=10^4$, i.e., 
$n_i=\sum_{j=1}^k y_{ij}=10^4$. We can use a STAN model (code shown below) 
to simulate data from $DM$ distribution:

  * $y \sim \mathrm{Multinomial}(\theta, n)$
  * $\theta \sim \mathrm{Dirichlet}(\alpha)$
  * $\alpha = \xi\cdot \mathrm{softmax}(\gamma + \beta x)$ 

where $x$ is a design variable, with $x=1$ for samples before and $x=-1$ 
for samples after treatment. $\gamma$ is intercept, $\beta$ is the effect 
of the treatment, and $\xi\in\mathbb{R}^{+}$ is precision (inverse of 
overdispersion).


```{stan, output.var = "model_sim"}
data {
  int<lower=0> K; // categories (genes)
  int<lower=0> N; // samples
  vector[K] gamma; // intercept
  real<lower=0> gamma_sigma; 
  vector[K] delta; // effect
  real<lower=0> delta_sigma;
  real xi; // precision
  int n; // total counts (tries)
}

generated quantities {
  int y_a [N,K]; // sample after
  int y_b [N,K]; // sample before
  vector [K] mu [2]; // intermediate alphas from dirichlet dist.
  vector [K] alpha [N]; 
  vector [K] beta [N];
  
  for(i in 1:N) {
   for(j in 1:K) {
     alpha[i][j] = normal_rng(gamma[j], gamma_sigma);
     beta[i][j] = normal_rng(delta[j], delta_sigma);
   } 
   mu[1] = dirichlet_rng(xi*softmax(alpha[i] + beta[i])); // before treatment sim. from DM.
   mu[2] = dirichlet_rng(xi*softmax(alpha[i] - beta[i])); // after treatment sim. from DM.
   y_a[i,] = multinomial_rng(mu[1], n); // simulate from multinomial dist. -> after sample
   y_b[i,] = multinomial_rng(mu[2], n); // simulate from multinomial dist. -> before sample 
  }
}

```

To simulate data we only need to provide the inputs (see 'data' block in the 
above STAN code):

```{r}
set.seed(seed = 123456)
K <- 7
N_train <- 10
N_test <- 10
N <- N_train + N_test
gamma <- rnorm(n = K, mean = 0, sd = 1)
delta <- c(rnorm(n = K-5, mean = 0, sd = 0.05), 
           rnorm(n = 5, mean = 0, sd = 0.35))

xi <- 500
n <- 10^4
gamma_sigma = 0.1
delta_sigma = 0.2
```

... and simulate 20 samples with:

```{r, echo = F}
sim_data <- rstan::sampling(
  object = model_sim,
  data = list(N = N, 
              K = K, 
              gamma = gamma, 
              delta = delta, 
              gamma_sigma = gamma_sigma,
              delta_sigma = delta_sigma,
              xi = xi, 
              n = n),
  chain = 1,
  cores = 1,
  iter = 1,
  warmup = 0,
  algorithm = "Fixed_param",
  seed = 12345,
  verbose = FALSE,
  refresh = -1)
```

We then extract the simulated observations: two matrices $\vec{y}^{a}$ and 
$\vec{y}^{b}$, with rows as samples and columns as genes. We treat the first 10 
samples (rows in each matrix) as observed/training samples ($D_{\text{train}}$), 
and we will treat the remaining 10 samples as unobserved/testing sample 
($D_{\text{test}}$).

```{r, echo=FALSE}
# extract simulated data
y_b <- rstan::extract(sim_data, par = "y_b")$y_b
y_a <- rstan::extract(sim_data, par = "y_a")$y_a

# D_train: observed data
y_b_o <- data.frame(reshape::melt(y_b[1,1:N_train, ]))
y_a_o <- data.frame(reshape::melt(y_a[1,1:N_train, ]))

# D_test: test data
y_b_t <- data.frame(reshape::melt(y_b[1,(N_train+1):(N_train+N_test), ]))
y_a_t <- data.frame(reshape::melt(y_a[1,(N_train+1):(N_train+N_test), ]))
```


```{r, echo = FALSE}
# format D_train
colnames(y_b_o) <- c("sample_id", "gene_name", "gene_usage_count")
y_b_o$condition <- "before"

colnames(y_a_o) <- c("sample_id", "gene_name", "gene_usage_count")
y_a_o$condition <- "after"

g_u <- rbind(y_b_o, y_a_o)
rm(y_a_o, y_b_o)
```


```{r, echo = FALSE}
# format  D_test
colnames(y_b_t) <- c("sample_id", "gene_name", "gene_usage_count")
y_b_t$condition <- "before"

colnames(y_a_t) <- c("sample_id", "gene_name", "gene_usage_count")
y_a_t$condition <- "after"

g_t <- rbind(y_b_t, y_a_t)
rm(y_a_t, y_b_t)
```


```{r, echo = FALSE}
# We will use IgGeneUsage’s utility function to structure ‘u’ into an 
# object that will make analysis convenient
get_paired_usage <- function(u) {
  u_t <- u[base::duplicated(u[, c("sample_id", "condition")])==FALSE,]
  u_t <- base::table(u_t$condition, u_t$sample_id)
  if(base::any(u_t!=1)) {
    stop(paste0("sample/s: ", 
                paste0(names(which(colSums(u_t)!=2)), collapse = ','),
                " not paired."))
  }
  
  # get Y data, fill empty combinations with 0
  Y <- reshape2::acast(data = u, 
                       formula = gene_name~sample_id+condition,
                       drop = FALSE, 
                       value.var = "gene_usage_count",
                       fill = 0, 
                       fun.aggregate = base::sum)
  Y <- reshape2::melt(Y)
  
  # processed data
  colnames(Y) <- c("gene_name", "sc", "gene_usage_count")
  k <- do.call(rbind, strsplit(x = as.character(Y$sc), split = '\\_'))
  Y$sample_id <- k[,1]
  Y$condition <- k[,2]
  Y$sc <- NULL
  
  # compute total usage
  N <- stats::aggregate(gene_usage_count~sample_id+condition, 
                        data = Y,
                        FUN = base::sum, 
                        drop = FALSE)
  N$total_usage_count <- N$gene_usage_count
  N$gene_usage_count <- NULL
  
  # merge usage and total usage
  Y <- base::merge(x = Y, y = N, 
                   by = c("sample_id", "condition"),
                   all.x = T)
  Y$gene_usage_prop <- Y$gene_usage_count/Y$total_usage_count
  Y$gene_name <- base::as.character(Y$gene_name)
  
  cs <- base::sort(base::unique(Y$condition))
  
  # get usage matrices
  Y_1 <- reshape2::acast(data = Y[Y$condition == cs[1], ], 
                         formula = gene_name~sample_id,
                         drop = FALSE, 
                         value.var = "gene_usage_count",
                         fill = 0, 
                         fun.aggregate = base::sum)
  Y_2 <- reshape2::acast(data = Y[Y$condition == cs[2], ], 
                         formula = gene_name~sample_id,
                         drop = FALSE, 
                         value.var = "gene_usage_count",
                         fill = 0, 
                         fun.aggregate = base::sum)
  
  if(ncol(Y_1)==1) {
    r_col <- base::colnames(Y_1)
    Y_1 <- Y_1[base::sort(base::rownames(Y_1)),]
    r <- matrix(data = Y_1, nrow = length(Y_1), ncol = 1)
    rownames(r) <- names(Y_1)
    colnames(r) <- r_col
    Y_1 <- r
    rm(r, r_col)
  } 
  else {
    Y_1 <- Y_1[base::sort(base::rownames(Y_1)), 
               base::sort(base::colnames(Y_1))]
  }
  if(ncol(Y_2)==1) {
    r_col <- base::colnames(Y_2)
    Y_2 <- Y_2[base::sort(base::rownames(Y_2)),]
    r <- matrix(data = Y_2, nrow = length(Y_2), ncol = 1)
    rownames(r) <- names(Y_2)
    colnames(r) <- r_col
    Y_2 <- r
    rm(r, r_col)
  } 
  else {
    Y_2 <- Y_2[base::sort(base::rownames(Y_2)), 
               base::sort(base::colnames(Y_2))]
  }
  
  N <- base::cbind(base::colSums(Y_1), 
                   base::colSums(Y_2))
  
  return (base::list(Y_1 = Y_1, 
                     Y_2 = Y_2, 
                     N = N, 
                     N_sample = base::ncol(Y_1), 
                     N_gene = base::nrow(Y_1), 
                     gene_names = base::rownames(Y_1),
                     sample_names = base::colnames(Y_1), 
                     proc_ud = Y))
}

get_usage <- function(u) {
  
  get_proc_ud <- function(Y, 
                          gene_names, 
                          sample_ids, 
                          X) {
    # process usage
    pud <- vector(mode = "list", length = ncol(Y))
    for(i in base::seq_len(length.out = ncol(Y))) {
      pud[[i]] <- data.frame(gene_usage_count = Y[,i],
                             gene_name = gene_names,
                             total_usage_count = sum(Y[,i]),
                             gene_usage_prop = Y[,i]/sum(Y[,i]),
                             sample_id = sample_ids[i],
                             condition = X[i],
                             stringsAsFactors = FALSE)
    }
    pud <- do.call(rbind, pud)
    rownames(pud) <- NULL
    return(pud)
  }
  
  
  
  # if the same sample_id is present in both conditions
  key <- paste(u$sample_id, u$condition, sep = '_')
  if(length(unique(key)) != length(unique(u$sample_id))) {
    warning("Same sample_id in both conditions, sample_id's extra coded")
    u$sample_id <- key
    rm(key)
  }
  
  # get Y data, fill empty combinations with 0
  Y <- reshape2::acast(data = u, 
                       formula = gene_name~sample_id,
                       drop = FALSE, 
                       value.var = "gene_usage_count",
                       fill = 0, 
                       fun.aggregate = sum)
  
  sample_ids <- colnames(Y)
  gene_names <- rownames(Y)
  
  # get N data
  N_d <- stats::aggregate(gene_usage_count~sample_id, 
                          data = u,
                          FUN = sum, 
                          drop = FALSE)
  N <- N_d$gene_usage_count
  names(N) <- N_d$sample_id
  rm(N_d)
  N <- N[sample_ids]
  
  # get X data
  u <-  u[u$sample_id %in% sample_ids, ]
  u <- u[duplicated(u[, c("sample_id")]) == FALSE, ]
  
  X_d <- stats::aggregate(condition~sample_id, 
                          data = u, 
                          FUN = unique)
  X <- X_d$condition
  names(X) <- X_d$sample_id
  rm(X_d)
  X <- X[sample_ids]
  
  # get X mapping
  X_u <- sort(x = unique(X), decreasing = TRUE)
  x1 <- which(X == X_u[1])
  x2 <- which(X == X_u[2])
  X_m <- numeric(length = length(X))
  X_m[x1] <- 1
  X_m[x2] <- -1
  
  # compute processed usage data
  pu <- get_proc_ud(Y = Y, 
                    gene_names = gene_names, 
                    sample_ids = sample_ids, 
                    X = X)
  
  return (list(Y = Y, 
               N = N, 
               N_sample = ncol(Y), 
               N_gene = nrow(Y),
               X = X_m, 
               Xorg = X, 
               gene_names = gene_names,
               sample_names = sample_ids, 
               proc_ud = pu))
}

```


```{r, echo = FALSE}
u <- get_paired_usage(u = g_u)
t <- get_paired_usage(u = g_t)

# add D_test data to object u
u$Y_1_test <- t$Y_1
u$Y_2_test <- t$Y_2
u$N_test <- t$N
u$N_sample_test <- t$N_sample
u$proc_ud_test <- t$proc_ud
rm(t)

# save for later use
save(u, file = "dm_sim/gene_usage_paired.RData", compress = T)
```


```{r, echo = FALSE}
g_u$sample_id <- paste0(g_u$sample_id, '_', g_u$condition)
g_t$sample_id <- paste0(g_t$sample_id, '_', g_t$condition)

u <- get_usage(u = g_u)
t <- get_usage(u = g_t)

# add D_test data to object u
u$Y_test <- t$Y
u$N_test <- t$N
u$X_test <- t$X
u$N_sample_test <- t$N_sample
u$proc_ud_test <- t$proc_ud
rm(t)

# save for later use
save(u, file = "dm_sim/gene_usage_unpaired.RData", compress = T)
```


```{r, echo=FALSE, eval=TRUE}
u <- get(load(file = "dm_sim/gene_usage_paired.RData"))
```


```{r, echo = FALSE}
str(u)
```

We can visualize the data using ggplot. Each point is an observation (mRNA 
count; y-axis) for a given gene (x-axis) from a given sample. Notice the 
differences in mRNA counts between the two conditions for genes 12, 5, 
etc. Train data are shown as filled points and test data as hollow points.

```{r, fig.width=5, fig.height=3, fig.align='center', eval=TRUE, echo=FALSE}
ggplot()+
  geom_point(data = u$proc_ud, aes(x = gene_name, y = gene_usage_count, col = condition),
             position = position_dodge(width = 0.5), size = 1)+
  geom_point(data = u$proc_ud_test, aes(x = gene_name, y = gene_usage_count, col = condition),
             position = position_dodge(width = 0.5), size = 1, shape = 21)+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  scale_color_manual(values = c("steelblue", "orange"))
```

## Modeling
We know that the data comes from $DM$ distribution. What would happen if we 
assumed that the data comes from:

  a) multinomial distribution (model $M$) and 
  b) dirichlet multinomial distribution (model $DM$)?

For this we will use two models that I have developed for differential 
immunoglobulin gene usage (DGU) analysis between pairs of samples. The 
models have *similar* structure as the model used for data simulation.

The main difference between the simulation model and the DGU models is that 
the DGU models are hierarchical. They assume that samples within a condition 
(here condition = "before" vs "after" treatment) are not completely independent
from each other, and use partial pooling to enable sharing of information 
(about parameters) between samples within a condition.

Model $DM$ and $M$ are identical, except for the following differences:

### Model $DM$

  * $\vec{y_i} \sim \mathrm{Multinomial}(\vec{\theta_i}, n_i)$
  * $\vec{\theta_i} \sim \mathrm{Dirichlet}(\xi \cdot \mathrm{softmax}(\vec{\mu_i}))$
  * $\vec{\mu_i} = \vec{\gamma}_i+\vec{\beta}_{i}x$

### Model $M$
  
  * $\vec{y_i} \sim \mathrm{Multinomial}(\vec{\theta_i}, n_i)$
  * $\vec{\theta_i} = \mathrm{softmax}(\vec{\mu_i})$
  * $\vec{\mu_i} = \vec{\gamma}_i+\vec{\beta}_{i}x$

### STAN code for model $DM$

```{stan, output.var = "model_dm", class.source = 'fold-hide'}
functions {
  real dirichlet_multinomial_complete_lpmf(int[] y, vector alpha) {
    real sum_alpha = sum(alpha);
    return lgamma(sum_alpha) - lgamma(sum(y) + sum_alpha)
           + lgamma(sum(y)+1) - sum(lgamma(to_vector(y)+1))
           + sum(lgamma(to_vector(y) + alpha)) - sum(lgamma(alpha));
  }
}

data {
  int <lower = 0> N_sample; // number of samples (repertoires)
  int <lower = 0> N_gene; // number of genes
  int Y_1 [N_gene, N_sample]; // number of successes (cells) in samples x gene
  int Y_2 [N_gene, N_sample]; // number of successes (cells) in samples x gene
  int N [N_sample, 2]; // number of total tries (repertoire size)
  // test data
  int <lower = 0> N_sample_test; // number of samples (repertoires)
  int Y_1_test [N_gene, N_sample_test]; // number of successes (cells) in samples x gene
  int Y_2_test [N_gene, N_sample_test]; // number of successes (cells) in samples x gene
  int N_test [N_sample_test, 2]; // number of total tries (repertoire size)
}

transformed data {
  real N_real [N_sample, 2];
  N_real = N;
}

parameters {
  vector [N_gene] alpha_mu_gene;
  real <lower=0> beta_sigma_gene;
  real <lower=0> alpha_sigma_gene;
  real <lower=0> beta_sigma_pop;
  vector [N_gene] beta_z [N_sample];
  vector [N_gene] alpha_z [N_sample];
  vector [N_gene] beta_z_gene;
  real <lower=0> xi;
}

transformed parameters {
  vector [N_gene] alpha [N_sample];
  vector [N_gene] beta [N_sample];
  vector [N_gene] beta_mu_gene;
  
  beta_mu_gene = 0+beta_sigma_pop*beta_z_gene;
  for(i in 1:N_sample) {
    beta[i] = beta_mu_gene + beta_sigma_gene * beta_z[i];
    alpha[i] = alpha_mu_gene + alpha_sigma_gene * alpha_z[i];
  }
}

model {
  target += exponential_lpdf(xi | 0.05);
  target += cauchy_lpdf(beta_sigma_pop | 0, 1);
  target += cauchy_lpdf(alpha_sigma_gene | 0, 1);
  target += cauchy_lpdf(beta_sigma_gene | 0, 1);
  for(i in 1:N_sample) {
    target += normal_lpdf(alpha_z[i] | 0, 1);
    target += normal_lpdf(beta_z[i] | 0, 1);
  }
  target += normal_lpdf(beta_z_gene | 0, 1);
  target += normal_lpdf(alpha_mu_gene | 0, 5);
  
  // likelihood
  for(i in 1:N_sample) {
    target += dirichlet_multinomial_complete_lpmf(Y_1[,i]|xi * softmax(alpha[i]-beta[i]));
    target += dirichlet_multinomial_complete_lpmf(Y_2[,i]|xi * softmax(alpha[i]+beta[i]));
  }
}

generated quantities {
  int Y_hat_1 [N_gene, N_sample];
  int Y_hat_2 [N_gene, N_sample];
  int Y_hat_group_1 [N_gene, N_sample];
  int Y_hat_group_2 [N_gene, N_sample];
  real log_lik [N_sample, 2];
  real log_lik_train [N_sample, 2];
  real log_lik_test [N_sample_test, 2];
  
  vector [N_gene] p [2];
  vector [N_gene] mu [2];
  real a [N_gene];
  real b [N_gene];

  for(i in 1:N_sample) {
    p[1] = dirichlet_rng(xi * softmax(alpha[i]-beta[i]));
    Y_hat_1[,i] = multinomial_rng(p[1], N[i,1]);
    p[2] = dirichlet_rng(xi * softmax(alpha[i]+beta[i]));
    Y_hat_2[,i] = multinomial_rng(p[2], N[i,2]);
    
    log_lik[i,1] = dirichlet_multinomial_complete_lpmf(Y_1[,i]|xi*softmax(alpha[i]-beta[i]));
    log_lik[i,2] = dirichlet_multinomial_complete_lpmf(Y_2[,i]|xi*softmax(alpha[i]+beta[i]));
  }
  
  // PPC: condition-specific
  a = normal_rng(alpha_mu_gene, alpha_sigma_gene);
  b = normal_rng(beta_mu_gene, beta_sigma_gene);
  mu[1] = xi * softmax(to_vector(a)-to_vector(b));
  mu[2] = xi * softmax(to_vector(a)+to_vector(b));
  // PPC: test
  for(i in 1:N_sample_test) {
    log_lik_test[i,1] = dirichlet_multinomial_complete_lpmf(Y_1_test[,i]|mu[1]);
    log_lik_test[i,2] = dirichlet_multinomial_complete_lpmf(Y_2_test[,i]|mu[2]);
  }
  // PPC: train
  for(i in 1:N_sample) {
    log_lik_train[i,1] = dirichlet_multinomial_complete_lpmf(Y_1[,i]|mu[1]);
    log_lik_train[i,2] = dirichlet_multinomial_complete_lpmf(Y_2[,i]|mu[2]);
    Y_hat_group_1[,i] = multinomial_rng(dirichlet_rng(mu[1]), N[i,1]);
    Y_hat_group_2[,i] = multinomial_rng(dirichlet_rng(mu[2]), N[i,2]);
  }
}
```


### STAN code for model $M$

```{stan, output.var = "model_m", class.source = 'fold-hide'}
data {
  int <lower = 0> N_sample; // number of samples (repertoires)
  int <lower = 0> N_gene; // number of genes
  int Y_1 [N_gene, N_sample]; // number of successes (cells) in samples x gene
  int Y_2 [N_gene, N_sample]; // number of successes (cells) in samples x gene
  int N [N_sample, 2]; // number of total tries (repertoire size)
  // test data
  int <lower = 0> N_sample_test; // number of samples (repertoires)
  int Y_1_test [N_gene, N_sample_test]; // number of successes (cells) in samples x gene
  int Y_2_test [N_gene, N_sample_test]; // number of successes (cells) in samples x gene
  int N_test [N_sample_test, 2]; // number of total tries (repertoire size)
}

transformed data {
  real N_real [N_sample, 2];
  N_real = N;
}

parameters {
  vector [N_gene] alpha_mu_gene;
  real <lower=0> beta_sigma_gene;
  real <lower=0> alpha_sigma_gene;
  real <lower=0> beta_sigma_pop;
  vector [N_gene] beta_z [N_sample];
  vector [N_gene] alpha_z [N_sample];
  vector [N_gene] beta_z_gene;
}

transformed parameters {
  vector [N_gene] alpha [N_sample];
  vector [N_gene] beta [N_sample];
  vector [N_gene] beta_mu_gene;
  
  beta_mu_gene = 0+beta_sigma_pop*beta_z_gene;
  for(i in 1:N_sample) {
    beta[i]=beta_mu_gene+beta_sigma_gene*beta_z[i];
    alpha[i]=alpha_mu_gene+alpha_sigma_gene*alpha_z[i];
  }
}

model {
  target += cauchy_lpdf(beta_sigma_pop | 0, 1);
  target += cauchy_lpdf(alpha_sigma_gene | 0, 1);
  target += cauchy_lpdf(beta_sigma_gene | 0, 1);
  for(i in 1:N_sample) {
    target += normal_lpdf(alpha_z[i] | 0, 1);
    target += normal_lpdf(beta_z[i] | 0, 1);
  }
  target += normal_lpdf(beta_z_gene | 0, 1);
  target += normal_lpdf(alpha_mu_gene | 0, 5);
  
  // likelihood
  for(i in 1:N_sample) {
    target += multinomial_lpmf(Y_1[,i] | softmax(alpha[i]-beta[i]));
    target += multinomial_lpmf(Y_2[,i] | softmax(alpha[i]+beta[i]));
  }
}

generated quantities {
  int Y_hat_1 [N_gene, N_sample];
  int Y_hat_2 [N_gene, N_sample];
  int Y_hat_group_1 [N_gene, N_sample];
  int Y_hat_group_2 [N_gene, N_sample];
  real log_lik [N_sample, 2];
  real log_lik_test [N_sample_test, 2];
  real log_lik_train [N_sample, 2];
  
  vector [N_gene] p [2];
  vector [N_gene] mu [2];
  real a [N_gene];
  real b [N_gene];
  
  // PPC
  for(i in 1:N_sample) {
    p[1] = softmax(alpha[i] - beta[i]);
    p[2] = softmax(alpha[i] + beta[i]);
    
    Y_hat_1[,i] = multinomial_rng(p[1], sum(Y_1[,i]));
    Y_hat_2[,i] = multinomial_rng(p[2], sum(Y_2[,i]));
    log_lik[i,1] = multinomial_lpmf(Y_1[,i] | p[1]);
    log_lik[i,2] = multinomial_lpmf(Y_2[,i] | p[2]);
  }
  
  // PPC - test (condition-specific)
  a = normal_rng(alpha_mu_gene, alpha_sigma_gene);
  b = normal_rng(beta_mu_gene, beta_sigma_gene);
  mu[1]=softmax(to_vector(a)-to_vector(b));
  mu[2]=softmax(to_vector(a)+to_vector(b));
  for(i in 1:N_sample_test) {
    log_lik_test[i,1] = multinomial_lpmf(Y_1_test[,i]|mu[1]);
    log_lik_test[i,2] = multinomial_lpmf(Y_2_test[,i]|mu[2]);
  }
  for(i in 1:N_sample) {
    log_lik_train[i,1] = multinomial_lpmf(Y_1[,i]|mu[1]);
    log_lik_train[i,2] = multinomial_lpmf(Y_2[,i]|mu[2]);
    Y_hat_group_1[,i] = multinomial_rng(mu[1], N[i,1]);
    Y_hat_group_2[,i] = multinomial_rng(mu[2], N[i,2]);
  }
}
```


### Model fitting
We will fit models $DM$ and $M$ using $D_{\text{train}}$.

```{r}
fit_dm <- rstan::sampling(object = model_dm,
                          data = u,
                          chains = 3,
                          cores = 3,
                          iter = 3500,
                          warmup = 1500,
                          control = list(adapt_delta = 0.95, 
                                         max_treedepth = 13),
                          algorithm = "NUTS")
```


```{r, echo = FALSE}
save(fit_dm, file = "dm_sim/fit_dm_paired.RData", compress = T)
```


```{r}
fit_m <- rstan::sampling(object = model_m,
                         data = u,
                         chains = 3,
                         cores = 3,
                         iter = 3500,
                         warmup = 1500,
                         control = list(adapt_delta = 0.95, 
                                        max_treedepth = 13),
                         algorithm = "NUTS")
```


```{r, echo = FALSE}
save(fit_m, file = "dm_sim/fit_m_paired.RData")
```


```{r, echo=FALSE, eval = TRUE, echo=F}
fit_dm <- get(load(file = "dm_sim/fit_dm_paired.RData"))
fit_m <- get(load(file = "dm_sim/fit_m_paired.RData"))
```


## Posterior predictive checks for each sample
We will use the parameters at the lowest level of the models, which are sample
specific parameters, to generate new data.

For model $DM$ predictions:
  
  * a = dirichlet_rng(GLM)
  * y_hat = multinomial_rng(a, n)

For model $M$ predictions:

  * y_hat = multinomial_rng(GLM, n)

These predictions (means and 95\% HDIs) will be compared against the observed 
data. Take a look at the predictions with narrow 95% HDIs by model $M$! 

```{r, eval = TRUE, echo = FALSE}

get_ppc <- function(fit, gene_usage, model_name) {
  summary_y_hat_1 <- data.frame(summary(fit, par = "Y_hat_1")$summary)
  summary_y_hat_1$Y <- as.vector(t(gene_usage$Y_1))
  x <- do.call(rbind, strsplit(x = gsub(pattern = "Y_hat_1|\\[|\\]", 
                                        replacement = '', 
                                        x = rownames(summary_y_hat_1)),
                               split = ','))
  summary_y_hat_1$gene_name <- x[, 1]
  summary_y_hat_1$sample_id <- x[, 2]
  summary_y_hat_1$condition <- "A"
  
  summary_y_hat_2 <- data.frame(summary(fit, par = "Y_hat_2")$summary)
  summary_y_hat_2$Y <-as.vector(t(gene_usage$Y_2))
  x <- do.call(rbind, strsplit(x = gsub(pattern = "Y_hat_2|\\[|\\]", 
                                        replacement = '', 
                                        x = rownames(summary_y_hat_2)),
                               split = ','))
  summary_y_hat_2$gene_name <- x[, 1]
  summary_y_hat_2$sample_id <- x[, 2]
  summary_y_hat_2$condition <- "B"
  
  s <- rbind(summary_y_hat_1, summary_y_hat_2)
  s$model_name <- model_name
  return(s)
}

```

```{r, eval = TRUE, echo = FALSE}
# DM
ppc_dm <- get_ppc(fit = fit_dm, gene_usage = u, model_name = "DM")

# M
ppc_m <- get_ppc(fit = fit_m, gene_usage = u, model_name = "M")

ppc <- rbind(ppc_dm, ppc_m)
rm(ppc_dm, ppc_m)
```

```{r, fig.width=8, fig.height=3.5, fig.align='center', eval=TRUE, echo=FALSE}
ggplot(data = ppc)+
  geom_abline(slope = 1, intercept = 0)+
  facet_grid(model_name~sample_id)+
  geom_point(aes(y = mean/10^3, x = Y/10^3, col = condition), size = 1)+
  geom_errorbar(aes(y = mean/10^3, x = Y/10^3, ymin = X2.5./10^3, 
                    ymax = X97.5./10^3, col = condition))+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  xlab(label = "Observed Y [in 10^3]")+
  ylab(label = "Predicted mean Y and 95% HDI [in 10^3]")+
  scale_color_manual(values = c("steelblue", "orange"))
```

Nearly all observations fall within the 95\% HDIs of the counts predicted by
either model.

```{r, eval = TRUE, echo = FALSE}
ppc$in_hdi <- ifelse(ppc$Y >= ppc$X2.5. & ppc$Y <= ppc$X97.5., 
                     yes = "in_hdi", no = "out_hdi")
table(ppc$in_hdi, ppc$model_name)
```


## PPC from condition-specific parameter sets
To check how well our models can predict the unobserved/test data 
($D_{\text{test}}$), we will generate (simulate) new data from each model,
and check whether the unobserved data falls within the 95\% HDI of the 
posterior of the simulated data points.

In general, we can simulate data in two ways using $M$ and $DM$:

  a. we can use the posteriors of the sample-specific parameters (1st layer 
     of parameters in the models) and draw random samples from
     multinomial or dirichlet-multinomial distribution
  
  
### Model $DM$

  * $\vec{y_i} \sim \mathrm{Multinomial}(\vec{\theta_i}, n_i)$
  * $\vec{\theta_i} \sim \mathrm{Dirichlet}(\xi \cdot \mathrm{softmax}(\vec{\mu_i}))$
  * $\vec{\mu_i} = \vec{\gamma}_i+\vec{\beta}_{i}x$

### Model $M$
  
  * $\vec{y_i} \sim \mathrm{Multinomial}(\vec{\theta_i}, n_i)$
  * $\vec{\theta_i} = \mathrm{softmax}(\vec{\mu_i})$
  * $\vec{\mu_i} = \vec{\gamma}_i+\vec{\beta}_{i}x$
  
However, in this case we cannot
simulate new data using the sample-specific parameters, i.e. for each observed sample we have a 
set of parameters ththese parameters correspond to each 
of the observed samples. For this, we have to use the condition-specific 
parameters found at the 2nd layer of model structure (see STAN code; generated 
quantities).

Now, the model $M$ predictions (means and 95\% HDIs) are associated with large 
degree of uncertainty, compensating for the too certain parameters from the 
sample-specific layer. However, many low count observations are outside the 
95\% HDIs of the predictions made by model $M$. Model $DM$ does a better 
prediction overall using its condition-specific parameters.

```{r, eval=TRUE, echo = FALSE}

get_ppc_group <- function(fit, gene_usage, model_name) {
  summary_y_hat_1 <- data.frame(summary(fit, par = "Y_hat_group_1")$summary)
  summary_y_hat_1$Y_Din <- as.vector(t(gene_usage$Y_1))
  summary_y_hat_1$Y_Dout <- as.vector(t(gene_usage$Y_1_test))
  
  x <- do.call(rbind, strsplit(x = gsub(pattern = "Y_hat_group_1|\\[|\\]", 
                                        replacement = '', 
                                        x = rownames(summary_y_hat_1)),
                               split = ','))
  summary_y_hat_1$gene_name <- x[, 1]
  summary_y_hat_1$sample_id <- x[, 2]
  summary_y_hat_1$condition <- "A"
  
  summary_y_hat_2 <- data.frame(summary(fit, par = "Y_hat_group_2")$summary)
  summary_y_hat_2$Y_Din <- as.vector(t(gene_usage$Y_2))
  summary_y_hat_2$Y_Dout <- as.vector(t(gene_usage$Y_2_test))
  x <- do.call(rbind, strsplit(x = gsub(pattern = "Y_hat_group_2|\\[|\\]", 
                                        replacement = '', 
                                        x = rownames(summary_y_hat_2)),
                               split = ','))
  summary_y_hat_2$gene_name <- x[, 1]
  summary_y_hat_2$sample_id <- x[, 2]
  summary_y_hat_2$condition <- "B"
  
  s <- rbind(summary_y_hat_1, summary_y_hat_2)
  s$model_name <- model_name
  return(s)
}

```

```{r, eval=TRUE, echo = FALSE}
# DM
ppc_group_dm <- get_ppc_group(fit = fit_dm, gene_usage = u, model_name = "DM")

# M
ppc_group_m <- get_ppc_group(fit = fit_m, gene_usage = u, model_name = "M")

ppc_group <- rbind(ppc_group_dm, ppc_group_m)
rm(ppc_group_dm, ppc_group_m)
```

```{r, fig.width=8, fig.height=5, fig.align='center', eval=TRUE, echo = FALSE}
(ggplot(data = ppc_group)+
  facet_wrap(facets = ~model_name, ncol = 1)+
  geom_point(aes(x = gene_name, y = mean, col = condition), 
                position = position_dodge(width = 0.55),  size = 0.5)+
  geom_point(aes(x = gene_name, y = Y_Din, group = condition),
                position = position_dodge(width = 0.55), size = 1.1)+
  geom_errorbar(aes(x = gene_name, y = mean, ymin = X2.5., ymax = X97.5., col = condition), 
                width = 0.35, position = position_dodge(width = 0.55))+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  xlab(label = "Gene")+
  ylab(label = "Predicted Y [95% HDI]")+
  scale_color_manual(values = c("steelblue", "orange"))+
   ggtitle(label = "D_train"))|
(ggplot(data = ppc_group)+
  facet_wrap(facets = ~model_name, ncol = 1)+
  geom_point(aes(x = gene_name, y = mean, col = condition), 
                position = position_dodge(width = 0.55),  size = 0.5)+
  geom_point(aes(x = gene_name, y = Y_Dout, group = condition),
                position = position_dodge(width = 0.55), size = 1.1)+
  geom_errorbar(aes(x = gene_name, y = mean, ymin = X2.5., ymax = X97.5., col = condition), 
                width = 0.35, position = position_dodge(width = 0.55))+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  xlab(label = "Gene")+
  ylab(label = "Predicted Y [95% HDI]")+
  scale_color_manual(values = c("steelblue", "orange"))+
   ggtitle(label = "D_test"))
```


### How many $D_{\text{train}}$ observations are outside the 95\% HDIs of the predictions?
Model $DM$ does more meaningful predictions with respect to both 
$D_{\text{test}}$ and $D_{\text{train}}$.

```{r, eval=TRUE, echo = FALSE}
ppc_group$in_hdi_in <- ifelse(ppc_group$Y_Din >= ppc_group$X2.5. & 
                             ppc_group$Y_Din <= ppc_group$X97.5., 
                           yes = "in_hdi", no = "out_hdi")
table(ppc_group$in_hdi_in, ppc_group$model_name)
```

### How many $D_{\text{test}}$ observations are outside the 95\% HDIs of the predictions?

```{r, eval=TRUE, echo = FALSE}
ppc_group$in_hdi_out <- ifelse(ppc_group$Y_Dout >= ppc_group$X2.5. & 
                             ppc_group$Y_Dout <= ppc_group$X97.5., 
                           yes = "in_hdi", no = "out_hdi")
table(ppc_group$in_hdi_out, ppc_group$model_name)
```


## Model comparison

### Widely Applicable Information Criterion (WAIC)
We compute the average log probability ($\text{lppd}$) for observation $y_i$ 
given posterior draw $s$ of the model parameters ($\Theta$): $\text{lppd}(y,\Theta)=\sum\limits_{i=1}^n\text{log}\dfrac{1}{S}\sum\limits_{s=1}^S \text{Pr}(y_i|\Theta_s)$. Moreover, we compute the effective number of 
parameters, $p_{\text{eff}}=\sum\limits_{i=1}^n V(y_i)$, where $V(y_i)$ is the 
variance of the log probability of observation $y_i$. Finally, 
$\text{WAIC} = -2(\text{lppd}-p_{\text{eff}})$, and we compute $\text{WAIC}$ for 
$D_{\text{train}}$ and $D_{\text{test}}$ for each model.

#### Model $DM$ WAIC

```{r, eval=TRUE, echo = FALSE}
loglik_train_dm <- summary(fit_dm, par = "log_lik_train")$summary
loglik_test_dm <- summary(fit_dm, par = "log_lik_test")$summary
```

#### $D_{\text{train}}$

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_train_dm[,"mean"]), "\n")
cat("p_eff:", sum(loglik_train_dm[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_train_dm[,"mean"])-sum(loglik_train_dm[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_train_dm[,"mean"])*sqrt(length(loglik_train_dm[,"mean"])))
# see https://avehtari.github.io/modelselection/CV-FAQ.html#15_Why_(sqrt{n})_in_Standard_error_(SE)_of_LOO
```

#### $D_{\text{test}}$
As expected, we see slightly worse (larger) \text{WAIC} for model $DM$ for 
$D_{\text{test}}$ compared to $D_{\text{train}}$:

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_test_dm[,"mean"]), "\n")
cat("p_eff:", sum(loglik_test_dm[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_test_dm[,"mean"])-sum(loglik_test_dm[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_test_dm[,"mean"])*sqrt(length(loglik_test_dm[,"mean"])))
```


#### Model $M$ WAIC
Notice that $\text{lppd}(D_{\text{train}})$ of model $M$ is much smaller 
than $\text{lppd}(D_{\text{train}})$ of model $DM$. This picture is consistent
with the accurate predictions of model $M$ with narrow 95\% HDIs.

```{r, eval=TRUE, echo = FALSE}
loglik_train_m <- summary(fit_m, par = "log_lik_train")$summary
loglik_test_m <- summary(fit_m, par = "log_lik_test")$summary
```

#### $D_{\text{train}}$

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_train_m[,"mean"]), "\n")
cat("p_eff:", sum(loglik_train_m[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_train_m[,"mean"])-sum(loglik_train_m[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_train_m[,"mean"])*sqrt(length(loglik_train_m[,"mean"])))
```

#### $D_{\text{test}}$
However, now have a look at the incredible high (bad) values for 
$\text{lppd}(D_{\text{test}})$ of model $M$ given $D_{\text{test}}$. We see 
especially high $p_{\text{eff}}$, which makes sense given the highly variable 
PPCs.

This indicates overfitting, i.e. model $M$ can predict accurately 
$D_{\text{train}}$ based on the sample-specific parameters, however, 
the model does not generalize well to new data. 

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_test_m[,"mean"]), "\n")
cat("p_eff:", sum(loglik_test_m[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_test_m[,"mean"])-sum(loglik_test_m[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_test_m[,"mean"])*sqrt(length(loglik_test_m[,"mean"])))
```


# Now the other way around: simulate data from $M$ distribution 

```{stan, output.var = "model_m_sim"}
data {
  int<lower=0> K; // categories (genes)
  int<lower=0> N; // samples
  vector[K] gamma; // intercept
  real<lower=0> gamma_sigma; 
  vector[K] delta; // effect
  real<lower=0> delta_sigma;
  int n; // total counts (tries)
}

generated quantities {
  int y_a [N,K]; // sample after
  int y_b [N,K]; // sample before
  vector [K] mu [2]; // intermediate alphas from dirichlet dist.
  vector [K] alpha [N]; 
  vector [K] beta [N];
  
  for(i in 1:N) {
   for(j in 1:K) {
     alpha[i][j] = normal_rng(gamma[j], gamma_sigma);
     beta[i][j] = normal_rng(delta[j], delta_sigma);
   } 
   mu[1] = softmax(alpha[i] + beta[i]); // before treatment sim. from DM.
   mu[2] = softmax(alpha[i] - beta[i]); // after treatment sim. from DM.
   y_a[i,] = multinomial_rng(mu[1], n); // simulate from multinomial dist. -> after sample
   y_b[i,] = multinomial_rng(mu[2], n); // simulate from multinomial dist. -> before sample 
  }
}

```

To simulate data we only need to provide the inputs (see 'data' block in the 
above STAN code):

```{r}
set.seed(seed = 12345)
K <- 7
N_train <- 10
N_test <- 10
N <- N_train + N_test
gamma <- rnorm(n = K, mean = 0, sd = 1)
delta <- c(rnorm(n = K-5, mean = 0, sd = 0.05), 
           rnorm(n = 5, mean = 0, sd = 0.25))
n <- 10^4

gamma_sigma = 0.001 # approx. complete pooling within gene -> very little noise
delta_sigma = 0.001 # approx. complete pooling within gene

```

... and simulate 20 samples with:

```{r, echo = F}
sim_data <- rstan::sampling(
  object = model_m_sim,
  data = list(N = N, 
              K = K, 
              gamma = gamma, 
              delta = delta, 
              gamma_sigma = gamma_sigma,
              delta_sigma = delta_sigma,
              n = n),
  chain = 1,
  cores = 1,
  iter = 1,
  warmup = 0,
  algorithm = "Fixed_param",
  seed = 12345,
  verbose = FALSE,
  refresh = -1)
```

We then extract the simulated observations: two matrices $\vec{y}^{a}$ and 
$\vec{y}^{b}$, with rows as samples and columns as genes. We treat the first 10 
samples (rows in each matrix) as observed/training samples ($D_{\text{train}}$), 
and we will treat the remaining 10 samples as unobserved/testing sample 
($D_{\text{test}}$).

```{r, echo=FALSE}
# extract simulated data
y_b <- rstan::extract(sim_data, par = "y_b")$y_b
y_a <- rstan::extract(sim_data, par = "y_a")$y_a

# D_train: observed data
y_b_o <- data.frame(reshape::melt(y_b[1,1:N_train, ]))
y_a_o <- data.frame(reshape::melt(y_a[1,1:N_train, ]))

# D_test: test data
y_b_t <- data.frame(reshape::melt(y_b[1,(N_train+1):(N_train+N_test), ]))
y_a_t <- data.frame(reshape::melt(y_a[1,(N_train+1):(N_train+N_test), ]))
```


```{r, echo = FALSE}
# format D_train
colnames(y_b_o) <- c("sample_id", "gene_name", "gene_usage_count")
y_b_o$condition <- "before"

colnames(y_a_o) <- c("sample_id", "gene_name", "gene_usage_count")
y_a_o$condition <- "after"

u <- rbind(y_b_o, y_a_o)
rm(y_a_o, y_b_o)
```


```{r, echo = FALSE}
# format  D_test
colnames(y_b_t) <- c("sample_id", "gene_name", "gene_usage_count")
y_b_t$condition <- "before"

colnames(y_a_t) <- c("sample_id", "gene_name", "gene_usage_count")
y_a_t$condition <- "after"

t <- rbind(y_b_t, y_a_t)
rm(y_a_t, y_b_t)
```




```{r, echo = FALSE}
u <- get_paired_usage(u = u)
t <- get_paired_usage(u = t)

# add D_test data to object u
u$Y_1_test <- t$Y_1
u$Y_2_test <- t$Y_2
u$N_test <- t$N
u$N_sample_test <- t$N_sample
u$proc_ud_test <- t$proc_ud
rm(t)

# save for later use
save(u, file = "dm_sim/gene_usage_m_paired.RData", compress = T)
```


```{r, echo=FALSE, eval=TRUE}
u <- get(load(file = "dm_sim/gene_usage_m_paired.RData"))
```


```{r, echo = FALSE}
str(u)
```

We can visualize the data using ggplot. Each point is the mRNA count (y-axis)
for a given gene (x-axis) of a sample (point). Notice the differences in 
mRNA counts between the two conditions for genes 12, 5, 7, etc. Filled points
are train and hollow points are test data.

```{r, fig.width=5, fig.height=2.5, fig.align='center', eval=TRUE, echo=FALSE}
ggplot()+
  geom_point(data = u$proc_ud, aes(x = gene_name, y = gene_usage_count, col = condition),
             position = position_dodge(width = 0.5), size = 1)+
  geom_point(data = u$proc_ud_test, aes(x = gene_name, y = gene_usage_count, col = condition),
             position = position_dodge(width = 0.5), size = 1, shape = 21)+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  scale_color_manual(values = c("steelblue", "orange"))
```

## Modeling
Same models, $DM$ and $M$, as before.

```{r}
fit_dm <- rstan::sampling(object = model_dm,
                          data = u,
                          chains = 3,
                          cores = 3,
                          iter = 3500,
                          warmup = 1500,
                          control = list(adapt_delta = 0.99, 
                                         max_treedepth = 14),
                          algorithm = "NUTS")
```


```{r, echo = FALSE}
save(fit_dm, file = "dm_sim/fit_dm_data_m_paired.RData", compress = T)
```



```{r}
fit_m <- rstan::sampling(object = model_m,
                         data = u,
                         chains = 3,
                         cores = 3,
                         iter = 3500,
                         warmup = 1500,
                         control = list(adapt_delta = 0.95, max_treedepth = 14),
                         algorithm = "NUTS")
```


```{r, echo = FALSE}
save(fit_m, file = "dm_sim/fit_m_data_m_paired.RData")
```


```{r, echo=FALSE, eval = TRUE, echo=F}
fit_dm <- get(load(file = "dm_sim/fit_dm_data_m_paired.RData"))
fit_m <- get(load(file = "dm_sim/fit_m_data_m_paired.RData"))
```


## Posterior predictive checks for each sample

```{r, eval = TRUE, echo = FALSE}

get_ppc <- function(fit, gene_usage, model_name) {
  summary_y_hat_1 <- data.frame(summary(fit, par = "Y_hat_1")$summary)
  summary_y_hat_1$Y <- as.vector(t(gene_usage$Y_1))
  x <- do.call(rbind, strsplit(x = gsub(pattern = "Y_hat_1|\\[|\\]", 
                                        replacement = '', 
                                        x = rownames(summary_y_hat_1)),
                               split = ','))
  summary_y_hat_1$gene_name <- x[, 1]
  summary_y_hat_1$sample_id <- x[, 2]
  summary_y_hat_1$condition <- "A"
  
  summary_y_hat_2 <- data.frame(summary(fit, par = "Y_hat_2")$summary)
  summary_y_hat_2$Y <-as.vector(t(gene_usage$Y_2))
  x <- do.call(rbind, strsplit(x = gsub(pattern = "Y_hat_2|\\[|\\]", 
                                        replacement = '', 
                                        x = rownames(summary_y_hat_2)),
                               split = ','))
  summary_y_hat_2$gene_name <- x[, 1]
  summary_y_hat_2$sample_id <- x[, 2]
  summary_y_hat_2$condition <- "B"
  
  s <- rbind(summary_y_hat_1, summary_y_hat_2)
  s$model_name <- model_name
  return(s)
}

```

```{r, eval = TRUE, echo = FALSE}
# DM
ppc_dm <- get_ppc(fit = fit_dm, gene_usage = u, model_name = "DM")

# M
ppc_m <- get_ppc(fit = fit_m, gene_usage = u, model_name = "M")

ppc <- rbind(ppc_dm, ppc_m)
rm(ppc_dm, ppc_m)
```

```{r, fig.width=9, fig.height=3.5, fig.align='center', eval=TRUE, echo=FALSE}
ggplot(data = ppc)+
  geom_abline(slope = 1, intercept = 0)+
  facet_grid(model_name~sample_id)+
  geom_point(aes(y = mean/10^3, x = Y/10^3, col = condition), size = 1)+
  geom_errorbar(aes(y = mean/10^3, x = Y/10^3, ymin = X2.5./10^3, 
                    ymax = X97.5./10^3, col = condition))+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  xlab(label = "Observed Y  (in 10^3)")+
  ylab(label = "Predicted Y and 95% HDI (in 10^3)")+
  scale_color_manual(values = c("steelblue", "orange"))
```


```{r, eval = TRUE, echo = FALSE}
ppc$in_hdi <- ifelse(ppc$Y >= ppc$X2.5. & ppc$Y <= ppc$X97.5., 
                     yes = "in_hdi", no = "out_hdi")
table(ppc$in_hdi, ppc$model_name)
```


## PPC from condition-specific parameters

```{r, eval=TRUE, echo = FALSE}
# DM
ppc_group_dm <- get_ppc_group(fit = fit_dm, gene_usage = u, model_name = "DM")

# M
ppc_group_m <- get_ppc_group(fit = fit_m, gene_usage = u, model_name = "M")

ppc_group <- rbind(ppc_group_dm, ppc_group_m)
rm(ppc_group_dm, ppc_group_m)
```

```{r, fig.width=8, fig.height=5, fig.align='center', eval=TRUE, echo = FALSE}
(ggplot(data = ppc_group)+
  facet_wrap(facets = ~model_name, ncol = 1)+
  geom_point(aes(x = gene_name, y = mean, col = condition), 
                position = position_dodge(width = 0.55),  size = 0.5)+
  geom_point(aes(x = gene_name, y = Y_Din, group = condition),
                position = position_dodge(width = 0.55), size = 1.1)+
  geom_errorbar(aes(x = gene_name, y = mean, ymin = X2.5., ymax = X97.5., col = condition), 
                width = 0.35, position = position_dodge(width = 0.55))+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  xlab(label = "Gene")+
  ylab(label = "Predicted Y [95% HDI]")+
  scale_color_manual(values = c("steelblue", "orange"))+
   ggtitle(label = "D_train"))|
(ggplot(data = ppc_group)+
  facet_wrap(facets = ~model_name, ncol = 1)+
  geom_point(aes(x = gene_name, y = mean, col = condition), 
                position = position_dodge(width = 0.55),  size = 0.5)+
  geom_point(aes(x = gene_name, y = Y_Dout, group = condition),
                position = position_dodge(width = 0.55), size = 1.1)+
  geom_errorbar(aes(x = gene_name, y = mean, ymin = X2.5., ymax = X97.5., col = condition), 
                width = 0.35, position = position_dodge(width = 0.55))+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  xlab(label = "Gene")+
  ylab(label = "Predicted Y [95% HDI]")+
  scale_color_manual(values = c("steelblue", "orange"))+
   ggtitle(label = "D_test"))
```


### How many $D_{\text{train}}$ observations are outside the 95\% HDIs of the predictions?

```{r, eval=TRUE, echo = FALSE}
ppc_group$in_hdi_in <- ifelse(ppc_group$Y_Din >= ppc_group$X2.5. & 
                             ppc_group$Y_Din <= ppc_group$X97.5., 
                           yes = "in_hdi", no = "out_hdi")
table(ppc_group$in_hdi_in, ppc_group$model_name)
```

### How many $D_{\text{test}}$ observations are outside the 95\% HDIs of the predictions?

```{r, eval=TRUE, echo = FALSE}
ppc_group$in_hdi_out <- ifelse(ppc_group$Y_Dout >= ppc_group$X2.5. & 
                             ppc_group$Y_Dout <= ppc_group$X97.5., 
                           yes = "in_hdi", no = "out_hdi")
table(ppc_group$in_hdi_out, ppc_group$model_name)
```



## Model comparison with the Widely Applicable Information Criterion (WAIC)

### Model $DM$ WAIC

```{r, eval=TRUE, echo = FALSE}
loglik_obs_dm <- summary(fit_dm, par = "log_lik")$summary
loglik_train_dm <- summary(fit_dm, par = "log_lik_train")$summary
loglik_test_dm <- summary(fit_dm, par = "log_lik_test")$summary
```


#### $D_{\text{train}}$
Notice that the loo package generated similar estimates:

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_train_dm[,"mean"]), "\n")
cat("p_eff:", sum(loglik_train_dm[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_train_dm[,"mean"])-sum(loglik_train_dm[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_train_dm[,"mean"])*sqrt(length(loglik_train_dm[,"mean"])))
# see https://avehtari.github.io/modelselection/CV-FAQ.html#15_Why_(sqrt{n})_in_Standard_error_(SE)_of_LOO
```

#### $D_{\text{test}}$

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_test_dm[,"mean"]), "\n")
cat("p_eff:", sum(loglik_test_dm[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_test_dm[,"mean"])-sum(loglik_test_dm[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_test_dm[,"mean"])*sqrt(length(loglik_test_dm[,"mean"])))
```


### Model $M$ WAIC

```{r, eval=TRUE, echo = FALSE}
loglik_obs_m <- summary(fit_m, par = "log_lik")$summary
loglik_train_m <- summary(fit_m, par = "log_lik_train")$summary
loglik_test_m <- summary(fit_m, par = "log_lik_test")$summary
```


#### $D_{\text{train}}$

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_train_m[,"mean"]), "\n")
cat("p_eff:", sum(loglik_train_m[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_train_m[,"mean"])-sum(loglik_train_m[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_train_m[,"mean"])*sqrt(length(loglik_train_m[,"mean"])))
```

#### $D_{\text{test}}$

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_test_m[,"mean"]), "\n")
cat("p_eff:", sum(loglik_test_m[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_test_m[,"mean"])-sum(loglik_test_m[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_test_m[,"mean"])*sqrt(length(loglik_test_m[,"mean"])))
```



